{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Programming Ex 01: Softmax Regression\n",
    "In this exercise you will:\n",
    "\n",
    "- Build the general architecture of a softmax regression algorithm, including:\n",
    "    - Initializing parameters\n",
    "    - Calculating the cost function and its gradient\n",
    "    - Using an optimization algorithm (gradient descent)\n",
    "    - Gather all three functions above into a main model function, in the right order.\n",
    "Instructions:\n",
    "\n",
    "- Read the comments carefully and run each cell (in the right order)\n",
    "- Implement the missing code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from utils.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR-10 visualization [0pt]\n",
    "\n",
    "Run the next cell to display a grid of example images.\n",
    "This ensures you can load the dataset - may take a few seconds to print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the raw CIFAR-10 data.\n",
    "\n",
    "cifar10_dir = '../../data/cifar/'\n",
    "X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "\n",
    "# Visualize some examples from the dataset.\n",
    "# We show a few examples of training images from each class.\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "num_classes = len(classes)\n",
    "samples_per_class = 7\n",
    "for y, cls in enumerate(classes):\n",
    "    idxs = np.flatnonzero(y_train == y)\n",
    "    idxs = np.random.choice(idxs, samples_per_class, replace=False)\n",
    "    for i, idx in enumerate(idxs):\n",
    "        plt_idx = i * num_classes + y + 1\n",
    "        plt.subplot(samples_per_class, num_classes, plt_idx)\n",
    "        plt.imshow(X_train[idx].astype('uint8'))\n",
    "        plt.axis('off')\n",
    "        if i == 0:\n",
    "            plt.title(cls)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR-10 load data [0pt]\n",
    "\n",
    "Run the next cell to define a data helper function that does some basic preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the linear classifier. These are the same steps as we used for the\n",
    "    SVM, but condensed to a single function.  \n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = '../../data/cifar'\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "    \n",
    "    # subsample the data\n",
    "    mask = list(range(num_training, num_training + num_validation))\n",
    "    # these are numpy arrays, not lists - that's why this indexing works.\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = list(range(num_training))\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = list(range(num_test))\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "    X_dev = X_train[mask]\n",
    "    y_dev = y_train[mask]\n",
    "    \n",
    "    # Preprocessing: reshape the image data into rows\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "    X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "    X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "    \n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis = 0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "    X_dev -= mean_image\n",
    "    \n",
    "    # add bias dimension and transform into columns\n",
    "    X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "    X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "    X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "    X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "print('dev data shape: ', X_dev.shape)\n",
    "print('dev labels shape: ', y_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Softmax - Loss Function and Gradients [5pt]\n",
    "\n",
    "Complete the implementation of softmax_loss_naive and implement a (naive) version of the (analytical) gradient descent that uses nested loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_loss_naive(W, X, y, reg):\n",
    "    \"\"\"\n",
    "    Softmax loss function, naive implementation (with loops)\n",
    "\n",
    "    Inputs have dimension D, there are C classes,\n",
    "    and we operate on minibatches of N examples.\n",
    "\n",
    "    Inputs:\n",
    "    - W: A numpy array of shape (D, C) containing weights.\n",
    "    - X: A numpy array of shape (N, D) containing a minibatch of data. 一行是一个sample\n",
    "    - y: A numpy array of shape (N,) containing training labels; y[i] = c means\n",
    "         that X[i] has label c, where 0 <= c < C. 所有点的真实class\n",
    "    - reg: (float) regularization strength\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - loss as single float\n",
    "    - gradient with respect to weights W; an array of same shape as W\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize the loss and gradient to zero.\n",
    "    loss = 0.0\n",
    "    dW = np.zeros_like(W)\n",
    "    \n",
    "    num_classes = W.shape[1]\n",
    "    num_samples = X.shape[0] #: #of training samples passed in batch X\n",
    "\n",
    "    #############################################################################\n",
    "    # TODO: Compute the softmax loss and its gradient using explicit loops.     #\n",
    "    # Store the loss in loss and the gradient in dW. If you are not careful     #\n",
    "    # here, it is easy to run into numeric instability. Don't forget the        #\n",
    "    # regularization!                                                           #\n",
    "    #############################################################################\n",
    "\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        pred = X[i].dot(W) # calcualte the prediction of data x[i]\n",
    "        pred -= np.max(pred)\n",
    "        prob_mat = np.exp(pred)/np.sum(np.exp(pred))\n",
    "        # calcualte the probability matrix of data x[i]\n",
    "                                 \n",
    "        for k in range(num_classes):\n",
    "            if y[i]==k:\n",
    "                loss -= np.log(prob_mat[k])\n",
    "                dW[:,k] += (prob_mat[k]-1.0)*X[i]\n",
    "            else:\n",
    "                dW[:,k] += prob_mat[k]*X[i]\n",
    "                \n",
    "    # regulization\n",
    "    loss = loss/num_samples +reg*0.5*np.sum(W*W)\n",
    "    dW = dW/num_samples + reg*W\n",
    "\n",
    "    #############################################################################\n",
    "    #                          END OF YOUR CODE                                 #\n",
    "    #############################################################################\n",
    "\n",
    "    return loss, dW\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check: Gradients [0pt]\n",
    "\n",
    "The next cell is a self-check for you to make sure your softmax implementation works!\n",
    "It compares your analytical gradients to numerically calculated gradients.\n",
    "The numerical and analytical values should be the same up to printing accurracy,\n",
    "relative error should be smaller than $10^{-7}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: -0.509473,\tanalytic: -0.509473,\trelative error: 6.240177e-08\n",
      "numerical: 0.519055,\tanalytic: 0.519055,\trelative error: 5.810709e-08\n",
      "numerical: 2.436275,\tanalytic: 2.436275,\trelative error: 1.368012e-08\n",
      "numerical: 1.242180,\tanalytic: 1.242180,\trelative error: 4.816156e-08\n",
      "numerical: 2.727981,\tanalytic: 2.727981,\trelative error: 3.840008e-09\n",
      "numerical: 2.005753,\tanalytic: 2.005753,\trelative error: 1.402382e-08\n",
      "numerical: -3.267406,\tanalytic: -3.267406,\trelative error: 9.705133e-09\n",
      "numerical: 0.515034,\tanalytic: 0.515034,\trelative error: 5.933524e-08\n",
      "numerical: -0.190648,\tanalytic: -0.190648,\trelative error: 1.613099e-07\n",
      "numerical: 2.455703,\tanalytic: 2.455703,\trelative error: 2.213591e-08\n",
      "numerical: 1.577892,\tanalytic: 1.577892,\trelative error: 4.807990e-09\n",
      "numerical: 2.098544,\tanalytic: 2.098544,\trelative error: 2.449199e-09\n",
      "numerical: -4.220163,\tanalytic: -4.220163,\trelative error: 2.446361e-08\n",
      "numerical: -0.657072,\tanalytic: -0.657072,\trelative error: 8.648244e-08\n",
      "numerical: -1.594433,\tanalytic: -1.594433,\trelative error: 3.508530e-08\n",
      "numerical: 0.186668,\tanalytic: 0.186668,\trelative error: 1.196985e-07\n",
      "numerical: 3.949016,\tanalytic: 3.949016,\trelative error: 1.959383e-08\n",
      "numerical: 2.463643,\tanalytic: 2.463643,\trelative error: 2.303985e-08\n",
      "numerical: -1.456149,\tanalytic: -1.456149,\trelative error: 3.345324e-09\n",
      "numerical: -2.046124,\tanalytic: -2.046124,\trelative error: 3.016302e-08\n"
     ]
    }
   ],
   "source": [
    "# Generate a random softmax weight matrix and use it to compute the loss.\n",
    "W = np.random.randn(3073, 10) * 0.0001 \n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# Use numeric gradient checking as a debugging tool.\n",
    "# The numeric gradient should be close to the analytic gradient.\n",
    "from utils.gradient_check import grad_check_sparse\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)\n",
    "\n",
    "# Do another gradient check with regularization\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 5e1)\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 5e1)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Softmax-  Loss Function & Gradients - Vectorized [5pt]\n",
    "\n",
    "Implement a vectorized version of the previous function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_loss_vectorized(W, X, y, reg):\n",
    "    \"\"\"\n",
    "    Softmax loss function, vectorized version.\n",
    "    Inputs have dimension D, there are C classes,\n",
    "    and we operate on minibatches of N examples.\n",
    "\n",
    "    Inputs:\n",
    "    - W: A numpy array of shape (D, C) containing weights.\n",
    "    - X: A numpy array of shape (N, D) containing a minibatch of data.\n",
    "    - y: A numpy array of shape (N,) containing training labels; y[i] = c means\n",
    "         that X[i] has label c, where 0 <= c < C.\n",
    "    - reg: (float) regularization strength\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - loss as single float\n",
    "    - gradient with respect to weights W; an array of same shape as W\n",
    "  \n",
    "  \n",
    "\n",
    "    Inputs and outputs are the same as softmax_loss_naive.\n",
    "    \"\"\"\n",
    "    # Initialize the loss and gradient to zero.\n",
    "    loss = 0.0\n",
    "    dW = np.zeros_like(W)\n",
    "    \n",
    "    num_samples = X.shape[0]\n",
    "    #############################################################################\n",
    "    # TODO: Compute the softmax loss and its gradient using no explicit loops.  #\n",
    "    # Store the loss in loss and the gradient in dW. If you are not careful     #\n",
    "    # here, it is easy to run into numeric instability. Don't forget the        #\n",
    "    # regularization!                                                           #\n",
    "    #############################################################################\n",
    "    pred = X.dot(W) # X(N,D) W(D,C) PRED(N,C)\n",
    "    # calcualte the prediction of all data, each row represents a prediction of a data point\n",
    "    pred -=np.max(pred,axis=1,keepdims=True) #pred(N,C)\n",
    "    prob_mat = np.exp(pred)/np.sum(np.exp(pred),axis=1,keepdims=True)# prob_mat(N,C) \n",
    "   \n",
    "    # each row represents the score of prediciton for one data point\n",
    "    corr_prob = prob_mat[range(num_samples),y] # return the score -> the data point is classifed correctly\n",
    "    # corr_prob(N,)\n",
    "    loss = -np.sum(np.log(corr_prob))/num_samples\n",
    "    \n",
    "    prob_mat[range(num_samples),y] -= 1 # I{yi = k} − sum(score)\n",
    "    dW = X.T.dot(prob_mat) #X(N,D) prob_mat(N,C) dW(D,C)           \n",
    "    # regulization\n",
    "    loss += reg*0.5*np.sum(W*W)\n",
    "    dW = dW/num_samples + reg*W\n",
    "\n",
    "    #############################################################################\n",
    "    #                          END OF YOUR CODE                                 #\n",
    "    #############################################################################\n",
    "\n",
    "    return loss, dW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check: Performance Comparison: [0pt]\n",
    "\n",
    "Execute the cell below to run a performance comparison between the naive implementation using loops\n",
    "and the optimized version using vectorized instructions.\n",
    "The two versions should compute the same results, but the vectorized version should be much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive loss: 2.354982e+00 computed in -0.096947s\n",
      "Vectorized loss: 2.354982e+00 computed in -0.006287s\n",
      "Loss difference: 0.000000\n",
      "Gradient difference: 0.000000\n",
      "Runtime abs. difference: 0.090660 seconds\n",
      "Runtime rel. improvement: 0.935152 percent\n"
     ]
    }
   ],
   "source": [
    "# The two versions should compute the same results, but the vectorized version should be\n",
    "# much faster.\n",
    "tic = time.time()\n",
    "loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "diff = tic - toc\n",
    "print('Naive loss: %e computed in %fs' % (loss_naive, diff))\n",
    "\n",
    "tic = time.time()\n",
    "loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "diff_vec = tic - toc\n",
    "print('Vectorized loss: %e computed in %fs' % (loss_vectorized, diff_vec))\n",
    "\n",
    "# As we did for the SVM, we use the Frobenius norm to compare the two versions\n",
    "# of the gradient.\n",
    "grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
    "print('Loss difference: %f' % np.abs(loss_naive - loss_vectorized))\n",
    "print('Gradient difference: %f' % grad_difference)\n",
    "print('Runtime abs. difference: %f seconds' % (diff_vec - diff))\n",
    "print('Runtime rel. improvement: %f percent' % (1 - (diff_vec / diff)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Softmax - Stochastic Gradient Descent [5pt]\n",
    "\n",
    "In the cell below, you have to utilize your previously implemented vectorized gradient function\n",
    "to train a softmax classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_SGD(X, y, learning_rate=1e-3, reg=1e-5, num_iters=100,\n",
    "            batch_size=200, verbose=False):\n",
    "    \"\"\"\n",
    "    Train this linear classifier using stochastic gradient descent.\n",
    "\n",
    "    Inputs:\n",
    "    - X: A numpy array of shape (N, D) containing training data; there are N\n",
    "      training samples each of dimension D.\n",
    "    - y: A numpy array of shape (N,) containing training labels; y[i] = c\n",
    "      means that X[i] has label 0 <= c < C for C classes.\n",
    "    - learning_rate: (float) learning rate for optimization.\n",
    "    - reg: (float) regularization strength.\n",
    "    - num_iters: (integer) number of steps to take when optimizing\n",
    "    - batch_size: (integer) number of training examples to use at each step.\n",
    "    - verbose: (boolean) If true, print progress during optimization.\n",
    "\n",
    "    Outputs:\n",
    "    W: A numpy array of shape (D, C) containing weights\n",
    "    loss_history: A list containing the value of the loss function at each training iteration.\n",
    "    \"\"\"\n",
    "    num_train, dim = X.shape\n",
    "    num_classes = np.max(y) + 1 # assume y takes values 0...K-1 where K is number of classes\n",
    "   \n",
    "    # Generate a random softmax weight matrix\n",
    "    W = 0.001 * np.random.randn(dim, num_classes)\n",
    "\n",
    "    # Run stochastic gradient descent to optimize W\n",
    "    loss_history = []\n",
    "    for it in range(num_iters):\n",
    "        X_batch = None\n",
    "        y_batch = None\n",
    "\n",
    "        #########################################################################\n",
    "        # TODO:                                                                 #\n",
    "        # Sample batch_size elements from the training data and their           #\n",
    "        # corresponding labels to use in this round of gradient descent.        #\n",
    "        # Store the data in X_batch and their corresponding labels in           #\n",
    "        # y_batch; after sampling X_batch should have shape (dim, batch_size)   #\n",
    "        # and y_batch should have shape (batch_size,)                           #\n",
    "        #                                                                       #\n",
    "        # Hint: Use np.random.choice to generate indices. Sampling with         #\n",
    "        # replacement is faster than sampling without replacement.              #\n",
    "        #########################################################################\n",
    "\n",
    "     \n",
    "        ind = np.random.choice(num_train,batch_size,replace = True)\n",
    "        X_batch = X[ind]\n",
    "        y_batch = y[ind]\n",
    "\n",
    "        #########################################################################\n",
    "        #                       END OF YOUR CODE                                #\n",
    "        #########################################################################\n",
    "\n",
    "        # evaluate loss and gradient\n",
    "        loss, grad = softmax_loss_vectorized(W, X_batch, y_batch, reg)\n",
    "        loss_history.append(loss)\n",
    "\n",
    "        # perform parameter update\n",
    "        #########################################################################\n",
    "        # TODO:                                                                 #\n",
    "        # Update the weights using the gradient and the learning rate.          #\n",
    "        #########################################################################\n",
    "        W -= grad*learning_rate\n",
    "        \n",
    "        #########################################################################\n",
    "        #                       END OF YOUR CODE                                #\n",
    "        #########################################################################\n",
    "\n",
    "        \n",
    "        if verbose and it % 100 == 0:\n",
    "            print('iteration %d / %d: loss %f' % (it, num_iters, loss))\n",
    "\n",
    "    return W, loss_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: Softmax - Predictor Function [5pt]\n",
    "\n",
    "In the next cell, you have to implement a function that predicts classes given a trained classifier and a\n",
    "batch of data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(W, X):\n",
    "    \"\"\"\n",
    "    Use the trained weights of this linear classifier to predict labels for\n",
    "    data points.\n",
    "\n",
    "    Inputs:\n",
    "    - W: A numpy array of shape (D, C) containing weights\n",
    "    - X: A numpy array of shape (N, D) containing training data; there are N\n",
    "      training samples each of dimension D.\n",
    "\n",
    "    Returns:\n",
    "    - y_pred: Predicted labels for the data in X. y_pred is a 1-dimensional\n",
    "      array of length N, and each element is an integer giving the predicted\n",
    "      class.\n",
    "    \"\"\"\n",
    "    y_pred = np.zeros(X.shape[0])\n",
    "    ###########################################################################\n",
    "    # TODO:                                                                   #\n",
    "    # Implement this method. Store the predicted labels in y_pred.            #\n",
    "    ###########################################################################\n",
    "    pred = X.dot(W) #  pred(N,C)\n",
    "    y_pred = np.argmax(pred,axis=1)\n",
    "   \n",
    "    ###########################################################################\n",
    "    #                           END OF YOUR CODE                              #\n",
    "    ###########################################################################\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check: Predictor Accuracy [0pt]\n",
    "\n",
    "If your code is correct, the next cell will give you an accuracy of about 33%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 2500: loss 384.612282\n",
      "iteration 100 / 2500: loss 232.494372\n",
      "iteration 200 / 2500: loss 141.351865\n",
      "iteration 300 / 2500: loss 86.218910\n",
      "iteration 400 / 2500: loss 52.898293\n",
      "iteration 500 / 2500: loss 32.891320\n",
      "iteration 600 / 2500: loss 20.581598\n",
      "iteration 700 / 2500: loss 13.271130\n",
      "iteration 800 / 2500: loss 8.862106\n",
      "iteration 900 / 2500: loss 6.229691\n",
      "iteration 1000 / 2500: loss 4.471467\n",
      "iteration 1100 / 2500: loss 3.527100\n",
      "iteration 1200 / 2500: loss 2.960658\n",
      "iteration 1300 / 2500: loss 2.558622\n",
      "iteration 1400 / 2500: loss 2.371605\n",
      "iteration 1500 / 2500: loss 2.240967\n",
      "iteration 1600 / 2500: loss 2.157878\n",
      "iteration 1700 / 2500: loss 2.141000\n",
      "iteration 1800 / 2500: loss 2.010347\n",
      "iteration 1900 / 2500: loss 2.109557\n",
      "iteration 2000 / 2500: loss 2.075025\n",
      "iteration 2100 / 2500: loss 1.957051\n",
      "iteration 2200 / 2500: loss 2.024487\n",
      "iteration 2300 / 2500: loss 2.114774\n",
      "iteration 2400 / 2500: loss 2.090432\n",
      "training accuracy: 0.3513061224489796\n"
     ]
    }
   ],
   "source": [
    "W, loss_hist = train_SGD(X_train, y_train, learning_rate=1e-7, reg=2.5e4, num_iters=2500,\n",
    "            batch_size=200, verbose=True)\n",
    "y_train_pred = predict(W,X_train)\n",
    "train_acc = np.mean(y_train == y_train_pred)\n",
    "print(\"training accuracy: \"+ str(train_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5: Softmax - Hyperparameter Tuning [5pt]\n",
    "\n",
    "Now let's put all function together and find the optimal hyperparameters \n",
    "(regularization strength and learning rate) using the validation set.\n",
    "You should experiment with different ranges for the learning\n",
    "rates and regularization strengths; if you are careful you should be able to\n",
    "get a classification accuracy of over 0.35 on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 2500: loss 384.772256\n",
      "iteration 100 / 2500: loss 232.831542\n",
      "iteration 200 / 2500: loss 141.367788\n",
      "iteration 300 / 2500: loss 86.279238\n",
      "iteration 400 / 2500: loss 52.891672\n",
      "iteration 500 / 2500: loss 32.782030\n",
      "iteration 600 / 2500: loss 20.635535\n",
      "iteration 700 / 2500: loss 13.290835\n",
      "iteration 800 / 2500: loss 8.885395\n",
      "iteration 900 / 2500: loss 6.130777\n",
      "iteration 1000 / 2500: loss 4.528724\n",
      "iteration 1100 / 2500: loss 3.578500\n",
      "iteration 1200 / 2500: loss 2.975046\n",
      "iteration 1300 / 2500: loss 2.611714\n",
      "iteration 1400 / 2500: loss 2.379930\n",
      "iteration 1500 / 2500: loss 2.248301\n",
      "iteration 1600 / 2500: loss 2.134979\n",
      "iteration 1700 / 2500: loss 2.134673\n",
      "iteration 1800 / 2500: loss 2.012867\n",
      "iteration 1900 / 2500: loss 2.062153\n",
      "iteration 2000 / 2500: loss 2.026133\n",
      "iteration 2100 / 2500: loss 2.125419\n",
      "iteration 2200 / 2500: loss 2.057670\n",
      "iteration 2300 / 2500: loss 2.071626\n",
      "iteration 2400 / 2500: loss 2.004607\n",
      "iteration 0 / 2500: loss 782.276454\n",
      "iteration 100 / 2500: loss 287.159514\n",
      "iteration 200 / 2500: loss 106.412756\n",
      "iteration 300 / 2500: loss 40.314006\n",
      "iteration 400 / 2500: loss 16.098896\n",
      "iteration 500 / 2500: loss 7.210308\n",
      "iteration 600 / 2500: loss 3.938374\n",
      "iteration 700 / 2500: loss 2.779588\n",
      "iteration 800 / 2500: loss 2.362260\n",
      "iteration 900 / 2500: loss 2.134102\n",
      "iteration 1000 / 2500: loss 2.139391\n",
      "iteration 1100 / 2500: loss 2.124540\n",
      "iteration 1200 / 2500: loss 2.157359\n",
      "iteration 1300 / 2500: loss 2.071185\n",
      "iteration 1400 / 2500: loss 2.118554\n",
      "iteration 1500 / 2500: loss 2.058613\n",
      "iteration 1600 / 2500: loss 2.128857\n",
      "iteration 1700 / 2500: loss 2.061803\n",
      "iteration 1800 / 2500: loss 2.025902\n",
      "iteration 1900 / 2500: loss 2.112800\n",
      "iteration 2000 / 2500: loss 2.107543\n",
      "iteration 2100 / 2500: loss 2.096118\n",
      "iteration 2200 / 2500: loss 2.138034\n",
      "iteration 2300 / 2500: loss 2.100541\n",
      "iteration 2400 / 2500: loss 2.034277\n",
      "iteration 0 / 2500: loss 390.959018\n",
      "iteration 100 / 2500: loss 32.931671\n",
      "iteration 200 / 2500: loss 4.556625\n",
      "iteration 300 / 2500: loss 2.268008\n",
      "iteration 400 / 2500: loss 2.044900\n",
      "iteration 500 / 2500: loss 2.067595\n",
      "iteration 600 / 2500: loss 1.992414\n",
      "iteration 700 / 2500: loss 2.068214\n",
      "iteration 800 / 2500: loss 2.076125\n",
      "iteration 900 / 2500: loss 2.019424\n",
      "iteration 1000 / 2500: loss 1.964283\n",
      "iteration 1100 / 2500: loss 2.006750\n",
      "iteration 1200 / 2500: loss 2.035199\n",
      "iteration 1300 / 2500: loss 1.983035\n",
      "iteration 1400 / 2500: loss 2.012511\n",
      "iteration 1500 / 2500: loss 2.087567\n",
      "iteration 1600 / 2500: loss 2.043748\n",
      "iteration 1700 / 2500: loss 2.061462\n",
      "iteration 1800 / 2500: loss 2.031883\n",
      "iteration 1900 / 2500: loss 1.996534\n",
      "iteration 2000 / 2500: loss 2.098886\n",
      "iteration 2100 / 2500: loss 2.033025\n",
      "iteration 2200 / 2500: loss 2.009878\n",
      "iteration 2300 / 2500: loss 2.083796\n",
      "iteration 2400 / 2500: loss 2.020628\n",
      "iteration 0 / 2500: loss 775.618970\n",
      "iteration 100 / 2500: loss 6.857129\n",
      "iteration 200 / 2500: loss 2.145056\n",
      "iteration 300 / 2500: loss 2.087726\n",
      "iteration 400 / 2500: loss 2.070581\n",
      "iteration 500 / 2500: loss 2.088013\n",
      "iteration 600 / 2500: loss 2.170568\n",
      "iteration 700 / 2500: loss 2.127795\n",
      "iteration 800 / 2500: loss 2.117785\n",
      "iteration 900 / 2500: loss 2.019020\n",
      "iteration 1000 / 2500: loss 2.040268\n",
      "iteration 1100 / 2500: loss 2.139039\n",
      "iteration 1200 / 2500: loss 2.033944\n",
      "iteration 1300 / 2500: loss 2.075759\n",
      "iteration 1400 / 2500: loss 2.041228\n",
      "iteration 1500 / 2500: loss 2.101448\n",
      "iteration 1600 / 2500: loss 2.124646\n",
      "iteration 1700 / 2500: loss 2.122371\n",
      "iteration 1800 / 2500: loss 2.051635\n",
      "iteration 1900 / 2500: loss 2.159308\n",
      "iteration 2000 / 2500: loss 2.114618\n",
      "iteration 2100 / 2500: loss 2.107173\n",
      "iteration 2200 / 2500: loss 2.151578\n",
      "iteration 2300 / 2500: loss 2.124736\n",
      "iteration 2400 / 2500: loss 2.123742\n",
      "lr 1.000000e-07 reg 2.500000e+04 train accuracy: 0.348143 val accuracy: 0.368000\n",
      "lr 1.000000e-07 reg 5.000000e+04 train accuracy: 0.326327 val accuracy: 0.338000\n",
      "lr 5.000000e-07 reg 2.500000e+04 train accuracy: 0.355776 val accuracy: 0.370000\n",
      "lr 5.000000e-07 reg 5.000000e+04 train accuracy: 0.328571 val accuracy: 0.352000\n",
      "best validation accuracy achieved during cross-validation: 0.370000\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "best_val = -1\n",
    "best_W = None\n",
    "learning_rates = [1e-7, 5e-7]\n",
    "regularization_strengths = [2.5e4, 5e4]\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Use the validation set to set the learning rate and regularization strength. #\n",
    "# the best trained softmax classifer in best_W.                                #\n",
    "################################################################################\n",
    "accu=0\n",
    "for i in learning_rates:\n",
    "    for j in regularization_strengths:\n",
    "        W, loss_hist = train_SGD(X_train, y_train, learning_rate=i, reg=j, num_iters=2500,batch_size=200, verbose=True)\n",
    "        y_train_pred = predict(W,X_train)\n",
    "        train_acc = np.mean(y_train == y_train_pred)\n",
    "        y_val_pred = predict(W,X_val)\n",
    "        val_acc = np.mean(y_val == y_val_pred)\n",
    "        results[(i,j)] = (train_acc,val_acc)\n",
    "        if best_val < val_acc:\n",
    "            best_val = val_acc\n",
    "            best_W = W\n",
    "        \n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################\n",
    "    \n",
    "# Print out results.\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print('lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
    "                lr, reg, train_accuracy, val_accuracy))\n",
    "    \n",
    "print('best validation accuracy achieved during cross-validation: %f' % best_val) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check: Prediction on Test Set [0pt]\n",
    "\n",
    "Check your models capabilities on unseen test data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax on raw pixels final test set accuracy: 0.362000\n"
     ]
    }
   ],
   "source": [
    "# evaluate on test set\n",
    "# Evaluate the best softmax on test set\n",
    "y_test_pred = predict(best_W,X_test)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print('softmax on raw pixels final test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
